{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62f8cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39aeafb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6996fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fmnist_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0ee948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,1:]\n",
    "Y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3aba416",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = np.array(x_train)/255.0 # scaling\n",
    "x_test = np.array(x_test)/255.0\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a4df6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,features,labels):\n",
    "        # convert row in 2d matrix\n",
    "        self.features = torch.tensor(features,dtype=torch.float32,device=device).reshape(-1,1,28,28)\n",
    "        self.labels = torch.tensor(labels,dtype=torch.long,device=device)\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    def __getitem__(self,index):\n",
    "        return self.features[index] , self.labels[index]\n",
    "\n",
    "train_dataset = CustomDataset(x_train,y_train)\n",
    "train_loder = DataLoader(dataset=train_dataset,batch_size=32,shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(x_test,y_test)\n",
    "test_loader = DataLoader(dataset=test_dataset,batch_size=32,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18f5912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySimpleNN(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_features,out_channels=32,kernel_size=3,padding='same',device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32), # improves cnn\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,padding='same',device=device),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )   \n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7,128,device=device),\n",
    "            nn.BatchNorm1d(128), # after layer before activation\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(128,64,device=device),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3), # afert RELU\n",
    "            nn.Linear(64,10,device=device) # soft max activation is internaly applied by defalut    \n",
    "        )\n",
    "    def forward(self, X):\n",
    "        X = self.features(X)\n",
    "        return self.classifier(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bed2006",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "epoches = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c9866e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MySimpleNN(1)\n",
    "\n",
    "model = model.to(device) # directly conver computation to GPU no need to make all tensors to GPU\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr = lr,weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86a111cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  --->  loss : tensor(0.8352, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  2  --->  loss : tensor(0.5129, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  3  --->  loss : tensor(0.4220, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  4  --->  loss : tensor(0.3619, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  5  --->  loss : tensor(0.3191, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  6  --->  loss : tensor(0.2745, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  7  --->  loss : tensor(0.2562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  8  --->  loss : tensor(0.2196, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  9  --->  loss : tensor(0.2027, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  10  --->  loss : tensor(0.1681, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  11  --->  loss : tensor(0.1558, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  12  --->  loss : tensor(0.1326, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  13  --->  loss : tensor(0.1113, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  14  --->  loss : tensor(0.1041, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  15  --->  loss : tensor(0.0979, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  16  --->  loss : tensor(0.0911, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  17  --->  loss : tensor(0.0818, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  18  --->  loss : tensor(0.0742, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  19  --->  loss : tensor(0.0779, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  20  --->  loss : tensor(0.0806, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  21  --->  loss : tensor(0.0592, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  22  --->  loss : tensor(0.0512, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  23  --->  loss : tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  24  --->  loss : tensor(0.0569, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  25  --->  loss : tensor(0.0484, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  26  --->  loss : tensor(0.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  27  --->  loss : tensor(0.0403, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  28  --->  loss : tensor(0.0368, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  29  --->  loss : tensor(0.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  30  --->  loss : tensor(0.0193, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  31  --->  loss : tensor(0.0266, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  32  --->  loss : tensor(0.0286, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  33  --->  loss : tensor(0.0262, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  34  --->  loss : tensor(0.0220, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  35  --->  loss : tensor(0.0242, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  36  --->  loss : tensor(0.0236, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  37  --->  loss : tensor(0.0220, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  38  --->  loss : tensor(0.0253, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  39  --->  loss : tensor(0.0223, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  40  --->  loss : tensor(0.0227, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  41  --->  loss : tensor(0.0173, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  42  --->  loss : tensor(0.0223, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  43  --->  loss : tensor(0.0214, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  44  --->  loss : tensor(0.0193, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  45  --->  loss : tensor(0.0314, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  46  --->  loss : tensor(0.0264, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  47  --->  loss : tensor(0.0170, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  48  --->  loss : tensor(0.0197, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  49  --->  loss : tensor(0.0165, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  50  --->  loss : tensor(0.0201, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  51  --->  loss : tensor(0.0259, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  52  --->  loss : tensor(0.0120, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  53  --->  loss : tensor(0.0125, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  54  --->  loss : tensor(0.0097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  55  --->  loss : tensor(0.0115, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  56  --->  loss : tensor(0.0146, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  57  --->  loss : tensor(0.0187, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  58  --->  loss : tensor(0.0136, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  59  --->  loss : tensor(0.0122, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  60  --->  loss : tensor(0.0090, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  61  --->  loss : tensor(0.0077, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  62  --->  loss : tensor(0.0132, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  63  --->  loss : tensor(0.0132, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  64  --->  loss : tensor(0.0171, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  65  --->  loss : tensor(0.0147, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  66  --->  loss : tensor(0.0137, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  67  --->  loss : tensor(0.0087, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  68  --->  loss : tensor(0.0075, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  69  --->  loss : tensor(0.0066, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  70  --->  loss : tensor(0.0161, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  71  --->  loss : tensor(0.0116, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  72  --->  loss : tensor(0.0147, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  73  --->  loss : tensor(0.0148, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  74  --->  loss : tensor(0.0160, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  75  --->  loss : tensor(0.0204, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  76  --->  loss : tensor(0.0201, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  77  --->  loss : tensor(0.0163, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  78  --->  loss : tensor(0.0107, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  79  --->  loss : tensor(0.0085, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  80  --->  loss : tensor(0.0093, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  81  --->  loss : tensor(0.0080, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  82  --->  loss : tensor(0.0112, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  83  --->  loss : tensor(0.0104, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  84  --->  loss : tensor(0.0141, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  85  --->  loss : tensor(0.0081, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  86  --->  loss : tensor(0.0087, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  87  --->  loss : tensor(0.0089, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  88  --->  loss : tensor(0.0090, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  89  --->  loss : tensor(0.0099, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  90  --->  loss : tensor(0.0058, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  91  --->  loss : tensor(0.0079, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  92  --->  loss : tensor(0.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  93  --->  loss : tensor(0.0072, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  94  --->  loss : tensor(0.0058, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  95  --->  loss : tensor(0.0087, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  96  --->  loss : tensor(0.0160, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  97  --->  loss : tensor(0.0058, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  98  --->  loss : tensor(0.0064, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  99  --->  loss : tensor(0.0047, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch:  100  --->  loss : tensor(0.0035, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoche in range(epoches):\n",
    "    epoch_avg_loss = 0\n",
    "    batch_size = 0\n",
    "    for batch_features , batch_labels in train_loder:\n",
    "        \n",
    "        batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
    "        \n",
    "        y_pred = model(batch_features)\n",
    "        \n",
    "        loss = loss_function(y_pred,batch_labels.long())\n",
    "        \n",
    "        epoch_avg_loss += loss\n",
    "        \n",
    "        batch_size +=1\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    print('Epoch: ', epoche+1,\" ---> \",\"loss :\",epoch_avg_loss/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20c284d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8766666666666667\n"
     ]
    }
   ],
   "source": [
    "# on test data\n",
    "model.eval()\n",
    "total = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch_features, batch_labels in test_loader:\n",
    "\n",
    "      outputs = model(batch_features) # ouput is 32X10 matrix  give for each image  prob that each will be\n",
    "\n",
    "      _, predicted = torch.max(outputs, 1) # max prob label is extracted\n",
    "\n",
    "      total = total + batch_labels.shape[0]\n",
    "\n",
    "      correct = correct + (predicted == batch_labels).sum().item()\n",
    "\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4f4170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
